!pip install "gymnasium[atari]"
!pip install autorom[accept-rom-license]
!pip install --upgrade gym pyvirtualdisplay ipykernel > /dev/null 2>&1
!pip install gymnasium ale-py

import gymnasium as gym
import ale_py
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Video, HTML
from IPython import display as ipythondisplay

gym.register_envs(ale_py)

# Создать окружающую среду
env = gym.make("MsPacman-v4", render_mode='rgb_array')

# Привести среду в исходное состояние перед началом
env.reset()

# Нарисовать первоначальное состояние игры
prev_screen = env.render()
plt.imshow(prev_screen)

# повторять 50 раз
for i in range(50):
    # предпринять случайное действие
    action = env.action_space.sample()
    obs, reward, done, truncated, info = env.step(action)

    # Нарисовать текущее состояние игры
    screen = env.render()
    plt.imshow(screen)

    # Очищаем вывод ячейки
    ipythondisplay.clear_output(wait=True)

    # Отрисовываем текущую фигуру
    ipythondisplay.display(plt.gcf())

    # Если шаг привел к завершению игры
    if done or truncated:
        break

# Очищаем вывод ячейки
ipythondisplay.clear_output(wait=True)

# Закрыть окружающую среду
env.close()

import gymnasium as gym
# создать и инициализировать окружающую среду
env = gym.make("MsPacman-v4", render_mode='rgb_array')
env.reset()

# список из вознаграждений
total_rewards = []
# сыграть 10 игр
n_episode = 10

for i in range(n_episode):
    # инициализировать переменные
    game_rew = 0
    while True:
        # выбрать случайное действие
        action = env.action_space.sample()
        # выполнить один шаг взаимодействия с окружающей средой
        new_obs, rew, done, truncated, _ = env.step(action)
        game_rew += rew
        # если завершено, напечатать полное вознаграждение в игре и сбросить среду
        if done or truncated:
            print('Эпизод %d завершен, Вознаграждение:%d' % (i, game_rew))
            env.reset()
            break
    total_rewards.append(game_rew)

print('Среднее полное вознаграждение в {} эпизодах: {}'.format(
          n_episode, sum(total_rewards) / n_episode))

import torch
x = torch.rand(3, 4)
print(x)

x = torch.rand(3, 4, dtype=torch.double)
print(x)

x = torch.zeros(3, 4)
print(x)

x = torch.ones(3, 4)
print(x)

print(x.size())

import torch.nn as nn
import torch.optim as optim

class SimpleNetwork(nn.Module):
    def __init__(self, n_state, n_action):
        super(SimpleNetwork, self).__init__()
        self.fc1 = nn.Linear(n_state, 128)
        self.fc2 = nn.Linear(128, n_action)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

import torch
import gymnasium as gym
env = gym.make("MsPacman-v4", render_mode='rgb_array')
env.reset()
n_state = 100800
n_action = env.action_space.n

# Инициализация сети
model = SimpleNetwork(n_state, n_action)
optimizer = optim.Adam(model.parameters(), lr=0.001)
# ε-жадная стратегия
epsilon = 0.1

def preprocess_state(state):
    # Преобразуем состояние в 1D-вектор
    state = state.flatten()
    return state

def run_episode_with_model(env, model):
    state, info = env.reset()
    total_reward = 0
    while True:
        state = preprocess_state(state)  # Преобразуем состояние
        state_tensor = torch.from_numpy(state).float()
        action_probs = model(state_tensor)
        action = torch.argmax(action_probs).item()
        state, reward, done, truncated, _ = env.step(action)
        total_reward += reward
        if done or truncated:
            break
    return total_reward


n_episode = 500

best_total_reward = 0
best_weight = torch.rand(n_state, n_action)

noise_scale = 0.01

# Основной цикл обучения
for episode in range(n_episode):
    total_reward = run_episode_with_model(env, model)
    print('Эпизод {}: {}'.format(episode + 1, total_reward))

print('Среднее полное вознаграждение в {} эпизодах: {}'.format(n_episode, sum(total_rewards) / n_episode))

noise_scale = 0.01
best_total_reward = 0
best_weight = [param.data.clone() for param in model.parameters()]
total_rewards = []
for episode in range(n_episode):
    # Добавляем шум к весам модели
    for i, param in enumerate(model.parameters()):
        noise = noise_scale * torch.randn_like(param.data)  # Генерируем шум
        param.data = best_weight[i] + noise  # Обновляем веса с шумом

    total_reward = run_episode_with_model(env, model)

    if total_reward >= best_total_reward:
        best_total_reward = total_reward
        best_weight = [param.data.clone() for param in model.parameters()]  # Сохраняем лучшие веса
        noise_scale = max(noise_scale / 1.2, 1e-4)  # Уменьшаем шум
    else:
        noise_scale = min(noise_scale * 1.1, 2)  # Увеличиваем шум

    print('Эпизод {}: {}'.format(episode + 1, total_reward))
    total_rewards.append(total_reward)

print('Лучшее вознаграждение: {}'.format(best_total_reward))


print('Среднее полное вознаграждение в {} эпизодах: {}'.format(n_episode, sum(total_rewards) / n_episode))

import matplotlib.pyplot as plt
plt.plot(total_rewards)
plt.xlabel('Эпизод')
plt.ylabel('Вознаграждение')
plt.show()


# Функция для установки весов модели
def set_model_weights(model, weights):
    for param, weight in zip(model.parameters(), weights):
        param.data.copy_(weight)

# Создаем модель
model_eval = model

n_episode_eval = 100
total_rewards_eval = []
# Устанавливаем лучшие веса в модель
set_model_weights(model_eval, best_weight)

for episode in range(n_episode_eval):
    total_reward = run_episode_with_model(env, model_eval)  # Используем модель с лучшими весами
    print('Эпизод {}: {}'.format(episode + 1, total_reward))
    total_rewards_eval.append(total_reward)


print('Среднее полное вознаграждение в {} эпизодах: {}'.format(n_episode_eval, sum(total_rewards_eval) / n_episode_eval))


# Функция для записи видео
def save_video(frames, filename):
    height, width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(filename, fourcc, 30, (width, height))

    for frame in frames:
        out.write(frame)
    out.release()




import cv2


# Запуск эпизода с записью видео
def run_episode_with_video(env, model):
    state, info = env.reset()
    total_reward = 0
    frames = []

    while True:
        state = preprocess_state(state)  # Убедитесь, что эта функция определена
        state_tensor = torch.from_numpy(state).float()
        action_probs = model(state_tensor)
        action = torch.argmax(action_probs).item()
        state, reward, done, truncated, _ = env.step(action)
        total_reward += reward

        frame = env.render()
        frames.append(frame)

        if done or truncated:
            break

    return total_reward, frames

# Запись видео с наилучшей моделью
video_filename = "best_episode.mp4"
total_reward, frames = run_episode_with_video(env, model_eval)
save_video(frames, video_filename)

print(f"Лучшее вознаграждение: {total_reward}")


# Функция для отображения видео
def display_video(filename):
    video_html = f'''
    <video width="640" height="480" controls>
        <source src="{filename}" type="video/mp4">
        Your browser does not support the video tag.
    </video>
    '''
    return HTML(video_html)


# Отобразите записанное видео
display_video(video_filename)

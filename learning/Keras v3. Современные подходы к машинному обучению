import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetV2B0

IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 5


!wget https://storage.yandexcloud.net/academy.ai/stanford_dogs.zip
# Разархивируем датасета во временную папку 'temp'
!unzip -qo "stanford_dogs" -d ./dataset

# Папка с папками картинок, рассортированных по категориям
IMAGE_PATH = './dataset/'


num_skipped = 0 # счетчик поврежденных файлов
for folder_name in os.listdir(IMAGE_PATH): # перебираем папки
    folder_path = os.path.join(IMAGE_PATH, folder_name) # склеиваем путь
    for fname in os.listdir(folder_path): # получаем список файлов в папке
        fpath = os.path.join(folder_path, fname) # получаем путь до файла
        try:
            fobj = open(fpath, "rb") # пытаемся открыть файл для бинарного чтения (rb)
            is_jfif = b"JFIF" in fobj.peek(10) # получаем первые 10 байт из файла и ищем в них бинарный вариант строки JFIF
        finally:
            fobj.close() # Закрываем файл

        if not is_jfif: # Если не нашли JFIF строку
            # Увеличиваем счетчик
            num_skipped += 1
            # Удаляем поврежденное изображение
            os.remove(fpath)

print(f"Удалено изображений: {num_skipped}")


# Загрузка данных
train_ds = keras.utils.image_dataset_from_directory(
    './dataset/',
    validation_split=0.2,
    subset="training",
    seed=42,
    shuffle=True,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)

val_ds = keras.utils.image_dataset_from_directory(
    './dataset/',
    validation_split=0.2,
    subset="validation",
    seed=42,
    shuffle=True,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)

# Сохранение классов до применения map и prefetch
class_names = train_ds.class_names
num_classes = len(class_names)

# Создание слоя аугментации изображений
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
])

# Применение аугментации к обучающему набору данных
train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y),
    num_parallel_calls=tf.data.AUTOTUNE,
)

# Включение предвыборки для повышения производительности
train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

# Создание и настройка модели
def build_model(num_classes):
    inputs = layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
    base_model = EfficientNetV2B0(include_top=False, input_tensor=inputs, weights="imagenet")

    # Заморозка всех слоев модели
    base_model.trainable = False

    x = layers.GlobalAveragePooling2D(name="avg_pool")(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs, outputs)

    # Компиляция модели
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )

    return model

# Тонкая настройка модели
model = build_model(num_classes)

# Заморозка только нижних слоев
for layer in model.layers[:-20]:
    layer.trainable = False

# Компиляция модели с уменьшенным learning rate для fine-tuning
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)



# Обучение модели с колбэками
callbacks = [
    keras.callbacks.ModelCheckpoint(filepath='best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max'),
]

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)

# Оценка и вывод результатов
loss, accuracy = model.evaluate(val_ds)
print(f'Validation accuracy: {accuracy:.2f}')


def unfreeze_model(model):
    # Размораживаем последние 20 слоев модели, за исключением BatchNormalization
    for layer in model.layers[-20:]:
        if not isinstance(layer, layers.BatchNormalization):
            layer.trainable = True
    # В оптимизаторе устанавливаем маленький шаг обучения
    optimizer = keras.optimizers.Adam(learning_rate=1e-5)
    model.compile(
        optimizer=optimizer, loss="sparse_categorical_crossentropy", metrics=["accuracy"]
    )
unfreeze_model(model)

history_fine_tune = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)

# Оценка модели после финальной настройки
loss, accuracy = model.evaluate(val_ds)
print(f'Validation accuracy after fine-tuning: {accuracy:.2f}')


import random
import shutil
import os
# Получаем список всех папок в директории
all_folders = [folder for folder in os.listdir(IMAGE_PATH) if os.path.isdir(os.path.join(IMAGE_PATH, folder))]

# Выбираем случайные 10 папок
selected_folders = random.sample(all_folders, 10)

# Удаляем все папки, которые не были выбраны
for folder in all_folders:
    if folder not in selected_folders:
        shutil.rmtree(os.path.join(IMAGE_PATH, folder))

print("Оставлены следующие породы:")
print(selected_folders)


# Пересоздаем датасеты после удаления файлов
train_ds = keras.utils.image_dataset_from_directory(
    IMAGE_PATH,
    validation_split=0.2,
    subset="training",
    seed=42,
    shuffle=True,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)

val_ds = keras.utils.image_dataset_from_directory(
    IMAGE_PATH,
    validation_split=0.2,
    subset="validation",
    seed=42,
    shuffle=True,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)

# Применяем аугментацию и предвыборку заново
train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y),
    num_parallel_calls=tf.data.AUTOTUNE,
)

train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

EPOCHS = 5
# Оценка модели после финальной настройки
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)
loss, accuracy = model.evaluate(val_ds)
print(f'Validation accuracy: {accuracy:.2f}')
